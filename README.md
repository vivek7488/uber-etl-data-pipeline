ğŸš• Uber ETL Pipeline â€“ End-to-End Data Engineering Project (AWS)
ğŸ“Œ Project Overview

This project demonstrates an end-to-end data engineering pipeline built using Uber trip data.
The pipeline ingests raw trip data, performs transformations using a star schema, stores analytics-ready data in a cloud data warehouse, and enables business insights through KPI queries and dashboards.

The project emphasizes ETL design, data modeling, cloud architecture, and analytics enablement.

ğŸ— ETL Pipeline Architecture

High-level flow:

Raw CSV Data â†’ Python ETL â†’ Amazon S3 â†’ Data Warehouse â†’ Analytics & KPIs

ğŸ›  Tech Stack

Programming Language: Python

Cloud Platform: Amazon Web Services (AWS)

Amazon S3 (Data Lake)

Amazon Redshift / Amazon Athena (Analytics)

Data Modeling: Star Schema

Analytics: SQL

Visualization: Amazon QuickSight / Looker Studio

Version Control: Git & GitHub

ğŸ“‚ Project Structure
uber-etl-pipeline/
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ uber_etl_pipeline.png
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ uber_trips.csv
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ analytics_table.sql
â”‚   â””â”€â”€ kpi_queries.sql
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ etl_transformation.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etl_pipeline.py
â”‚
â””â”€â”€ README.md

ğŸ“Š Data Model

The project follows a star schema optimized for analytical queries.

Fact Table

fact_table

trip_id

vendor_id

datetime_id

passenger_count_id

trip_distance_id

rate_code_id

pickup_location_id

dropoff_location_id

payment_type_id

fare and revenue metrics

Dimension Tables

datetime_dim

passenger_count_dim

trip_distance_dim

rate_code_dim

pickup_location_dim

dropoff_location_dim

payment_type_dim

âš™ï¸ ETL Pipeline Details
1ï¸âƒ£ Extract

Raw Uber trip data ingested from CSV files.

2ï¸âƒ£ Transform

Data cleaning and normalization

Type casting and validation

Star schema modeling (fact & dimensions)

Analytics table creation using SQL joins

3ï¸âƒ£ Load

Raw data stored in Amazon S3

Transformed data loaded into Redshift / Athena

Analytics-ready tables created for BI usage

ğŸ“ˆ Analytics Layer

An analytics table (tbl_analytics) is created by joining the fact table with all dimension tables.

This table enables:

Fast analytical queries

KPI calculations

Time-based and revenue-based insights

All analytics SQL logic is maintained in the sql/ directory.

ğŸ“Œ Key Business KPIs

The following KPIs are derived from the analytics table:

Total Trips

Total Revenue

Average Fare per Trip

Revenue per Trip

Trips & Revenue by Payment Type

Average Trip Distance

Peak Pickup Hours

Daily Trips & Revenue Trends

Average Tip Percentage

Revenue by Passenger Count

ğŸš€ How to Run the Project
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/uber-etl-pipeline.git
cd uber-etl-pipeline

2ï¸âƒ£ AWS Setup

Create an Amazon S3 bucket for raw data

Configure Amazon Redshift or Athena

Upload raw CSV data to S3

3ï¸âƒ£ Run ETL

Execute Python scripts or notebooks to generate fact and dimension tables

Run SQL scripts from the sql/ folder to create analytics and KPI layers
